#!/usr/bin/env python3

"""
Monte Carlo was the first task, it can be quick but TD updates in real time and
although more expensive, seems to require less episodes. It is also the name
of this project I mean come on! 
"""

import numpy as np


def td_lambtha(
    env,
    V,
    policy,
    lambtha,
    episodes=5000,
    max_steps=100,
    alpha=0.1,
    gamma=0.99
):
    """
    Performs the TD(Î») algorithm for value estimation.

    Args:
        env (object): The environment instance (expected to be a Gymnasium Env).
        V (numpy.ndarray): A numpy.ndarray of shape (s,) containing the
            value estimate for each state.
        policy (function): A function that takes in a state and returns
            the next action to take.
        lambtha (float): The eligibility trace factor.
        episodes (int, optional): The total number of episodes to train over.
            Defaults to 5000.
        max_steps (int, optional): The maximum number of steps per episode.
            Defaults to 100.
        alpha (float, optional): The learning rate. Defaults to 0.1.
        gamma (float, optional): The discount rate. Defaults to 0.99.

    Returns:
        numpy.ndarray: V, the updated value estimate.
    """
    
    for episode in range(episodes):
        state, _ = env.reset()
        e_traces = np.zeros_like(V)

        for step in range(max_steps):
            action = policy(state)

            next_state, reward, terminated, truncated, _ = env.step(action)

            done = terminated or truncated

            td_error = reward + gamma * V[next_state] - V[state]

            e_traces[state] += 1

            V += alpha * td_error * e_traces

            e_traces *= gamma * lambtha

            state = next_state

            if done:
                break

    return V
