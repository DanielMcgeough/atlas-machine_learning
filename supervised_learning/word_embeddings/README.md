This project explores various techniques within Natural Language Processing (NLP) for representing words numerically, focusing on methods that capture semantic relationships. One key approach discussed is word embeddings, which transform words into dense vectors in a continuous space, allowing for the measurement of similarity and analogy. ELMo (Embeddings from Language Models) is presented as a breakthrough, utilizing deep bidirectional LSTMs to create contextualized word representations that achieved state-of-the-art results across multiple NLP tasks. Traditional methods like Bag of Words and TF-IDF are contrasted with more advanced techniques such as Word2Vec and GloVe, highlighting the progression towards capturing deeper semantic meaning. Word2Vec, using models like Skip-Gram and CBOW, learns embeddings by predicting surrounding words, while GloVe leverages global word co-occurrence statistics. The concept of negative sampling as an optimization for training word embeddings is also introduced. Finally, the broader field of NLP, its history, common tasks, and future directions involving symbolic, statistical, and neural network approaches are outlined.
